{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b7e372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6852308b00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ee380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_customer_data = spark.read.json(\"file:///home/hadoop/Downloads/bank_edited.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d162becf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+\n",
      "|age|balance|campaign|contact|day|default|duration|education|housing|         job|loan| marital|month|pdays|poutcome|previous|  y|\n",
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+\n",
      "| 58|   2143|       1|unknown|  5|     no|     261| tertiary|    yes|  management|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 44|     29|       1|unknown|  5|     no|     151|secondary|    yes|  technician|  no|  single|  may|   -1| unknown|       0| no|\n",
      "| 33|      2|       1|unknown|  5|     no|      76|secondary|    yes|entrepreneur| yes| married|  may|   -1| unknown|       0| no|\n",
      "| 47|   1506|       1|unknown|  5|     no|      92|  unknown|    yes| blue-collar|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 33|      1|       1|unknown|  5|     no|     198|  unknown|     no|     unknown|  no|  single|  may|   -1| unknown|       0| no|\n",
      "| 35|    231|       1|unknown|  5|     no|     139| tertiary|    yes|  management|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 28|    447|       1|unknown|  5|     no|     217| tertiary|    yes|  management| yes|  single|  may|   -1| unknown|       0| no|\n",
      "| 42|      2|       1|unknown|  5|    yes|     380| tertiary|    yes|entrepreneur|  no|divorced|  may|   -1| unknown|       0| no|\n",
      "| 58|    121|       1|unknown|  5|     no|      50|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 43|    593|       1|unknown|  5|     no|      55|secondary|    yes|  technician|  no|  single|  may|   -1| unknown|       0| no|\n",
      "| 41|    270|       1|unknown|  5|     no|     222|secondary|    yes|      admin.|  no|divorced|  may|   -1| unknown|       0| no|\n",
      "| 29|    390|       1|unknown|  5|     no|     137|secondary|    yes|      admin.|  no|  single|  may|   -1| unknown|       0| no|\n",
      "| 53|      6|       1|unknown|  5|     no|     517|secondary|    yes|  technician|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 58|     71|       1|unknown|  5|     no|      71|  unknown|    yes|  technician|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 57|    162|       1|unknown|  5|     no|     174|secondary|    yes|    services|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 51|    229|       1|unknown|  5|     no|     353|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 45|     13|       1|unknown|  5|     no|      98|  unknown|    yes|      admin.|  no|  single|  may|   -1| unknown|       0| no|\n",
      "| 57|     52|       1|unknown|  5|     no|      38|  primary|    yes| blue-collar|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 60|     60|       1|unknown|  5|     no|     219|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no|\n",
      "| 33|      0|       1|unknown|  5|     no|      54|secondary|    yes|    services|  no| married|  may|   -1| unknown|       0| no|\n",
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_customer_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe230511",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- campaign: long (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- pdays: long (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- previous: long (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_customer_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22f44c",
   "metadata": {},
   "source": [
    "#### 1.Display Max, Min and Mean Age of targeted customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae613e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_customer_data.createOrReplaceTempView(\"banktable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b96208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "882760bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      95|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|      18|\n",
      "+--------+\n",
      "\n",
      "+------------------+\n",
      "|round(avg(age), 0)|\n",
      "+------------------+\n",
      "|              41.0|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_customer_data.select(max(\"age\")).show()\n",
    "bank_customer_data.select(min(\"age\")).show()\n",
    "bank_customer_data.select(round(mean(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f38d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+\n",
      "|max(age)|min(age)|mean|\n",
      "+--------+--------+----+\n",
      "|      95|      18|41.0|\n",
      "+--------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql - alternative method\n",
    "spark.sql(\"select max(age), min(age), round(mean(age)) mean from banktable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955db413",
   "metadata": {},
   "source": [
    "#### 2. Check tge quality of customers by analyzing targeted customers min, max, mean, median, balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dfb73f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max(balance)|\n",
      "+------------+\n",
      "|      102127|\n",
      "+------------+\n",
      "\n",
      "+------------+\n",
      "|min(balance)|\n",
      "+------------+\n",
      "|       -8019|\n",
      "+------------+\n",
      "\n",
      "+----------------------+\n",
      "|round(avg(balance), 0)|\n",
      "+----------------------+\n",
      "|                1362.0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_customer_data.select(max(\"balance\")).show()\n",
    "bank_customer_data.select(min(\"balance\")).show()\n",
    "bank_customer_data.select(round(mean(\"balance\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9fa1926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+------+------+\n",
      "|max(balance)|min(abs(balance))|  mean|median|\n",
      "+------------+-----------------+------+------+\n",
      "|      102127|                0|1362.0|   448|\n",
      "+------------+-----------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select max(balance), min(abs(balance)), round(mean(balance)) mean, percentile_approx(balance,0.5) median from banktable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ee8c3",
   "metadata": {},
   "source": [
    "#### 3. Check if age matters in marketing subscription for term Deposit scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0e84d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "|age|customer_count|\n",
      "+---+--------------+\n",
      "| 32|           221|\n",
      "| 30|           217|\n",
      "| 33|           210|\n",
      "| 35|           209|\n",
      "| 31|           206|\n",
      "| 34|           198|\n",
      "| 36|           195|\n",
      "| 29|           171|\n",
      "| 37|           170|\n",
      "| 28|           162|\n",
      "| 38|           144|\n",
      "| 39|           143|\n",
      "| 27|           141|\n",
      "| 26|           134|\n",
      "| 41|           120|\n",
      "| 46|           118|\n",
      "| 40|           116|\n",
      "| 25|           113|\n",
      "| 47|           113|\n",
      "| 42|           111|\n",
      "+---+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select age, count(*) as customer_count from banktable where y = 'yes' group by age order by \\\n",
    "          customer_count desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14614f",
   "metadata": {},
   "source": [
    "#### 4. Calculate the marketing success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4107566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      success_rate|\n",
      "+------------------+\n",
      "|11.698480458295547|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select (select count(*) from banktable where y = 'yes') / count(*) * 100 success_rate from banktable \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b78aac",
   "metadata": {},
   "source": [
    "#### 5. Calculate marketing failure rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e72d8ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|failure_rate|\n",
      "+------------+\n",
      "|      88.302|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select round((select count(*) from banktable where y = 'no') / count(*) * 100,3) failure_rate from banktable \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5267b0",
   "metadata": {},
   "source": [
    "#### 6. check if marital status matters in marketing subscription for term deposit scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dd6ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "| marital|customer_count|\n",
      "+--------+--------------+\n",
      "| married|          2755|\n",
      "|  single|          1912|\n",
      "|divorced|           622|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select marital, count(*) as customer_count from banktable \n",
    "where y = 'yes' \n",
    "group by marital \n",
    "order by customer_count desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef2fc3",
   "metadata": {},
   "source": [
    "#### 8. check if age and marital status together mattered for subscription to term deposit scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0288659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------+\n",
      "|age|marital|customer_count|\n",
      "+---+-------+--------------+\n",
      "| 30| single|           151|\n",
      "| 28| single|           138|\n",
      "| 29| single|           133|\n",
      "| 32| single|           124|\n",
      "| 26| single|           121|\n",
      "| 34|married|           118|\n",
      "| 31| single|           111|\n",
      "| 27| single|           110|\n",
      "| 35|married|           101|\n",
      "| 36|married|           100|\n",
      "| 25| single|            99|\n",
      "| 37|married|            98|\n",
      "| 33|married|            97|\n",
      "| 33| single|            97|\n",
      "| 39|married|            87|\n",
      "| 32|married|            87|\n",
      "| 38|married|            86|\n",
      "| 35| single|            84|\n",
      "| 47|married|            83|\n",
      "| 46|married|            80|\n",
      "+---+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select age,marital, count(*) as customer_count from banktable \n",
    "where y = 'yes' \n",
    "group by age, marital \n",
    "order by customer_count desc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd03cf3",
   "metadata": {},
   "source": [
    "#### 9. Compute success rate of each age and status category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60a6f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-------------+------------------+\n",
      "|age| marital|total|count_success|success_percentage|\n",
      "+---+--------+-----+-------------+------------------+\n",
      "| 68|divorced|    6|            6|             100.0|\n",
      "| 95|divorced|    1|            1|             100.0|\n",
      "| 93| married|    2|            2|             100.0|\n",
      "| 92| married|    2|            2|             100.0|\n",
      "| 90|divorced|    2|            2|             100.0|\n",
      "| 85|divorced|    1|            1|             100.0|\n",
      "| 87|divorced|    1|            1|             100.0|\n",
      "| 86|  single|    1|            1|             100.0|\n",
      "| 67|divorced|    8|            7|              87.5|\n",
      "| 62|divorced|    6|            5| 83.33333333333333|\n",
      "| 85| married|    4|            3|              75.0|\n",
      "| 76|divorced|    8|            6|              75.0|\n",
      "| 71|divorced|   11|            8| 72.72727272727273|\n",
      "| 87| married|    3|            2| 66.66666666666667|\n",
      "| 84| married|    6|            4| 66.66666666666667|\n",
      "| 73|divorced|    6|            4| 66.66666666666667|\n",
      "| 77|divorced|    5|            3|              60.0|\n",
      "| 18|  single|   12|            7|58.333333333333336|\n",
      "| 63|divorced|    7|            4|57.142857142857146|\n",
      "| 73| married|   36|           19| 52.77777777777778|\n",
      "+---+--------+-----+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select age,marital, count(*) as total,\n",
    "        SUM(case when y = 'yes' then 1 else 0 end) as count_success,\n",
    "        SUM(case when y = 'yes' then 1 else 0 end)*100 / count(*) as success_percentage\n",
    "    from banktable\n",
    "group by age, marital order by success_percentage desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c2cd683",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input ')' expecting <EOF>(line 3, pos 70)\\n\\n== SQL ==\\n\\n    select age,marital, count(*) as total,\\n        (select count(*) from banktable where y = 'yes')/count(*) *100)\\n----------------------------------------------------------------------^^^\\n\\n    from banktable\\ngroup by age, marital\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input ')' expecting <EOF>(line 3, pos 70)\n\n== SQL ==\n\n    select age,marital, count(*) as total,\n        (select count(*) from banktable where y = 'yes')/count(*) *100)\n----------------------------------------------------------------------^^^\n\n    from banktable\ngroup by age, marital\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:69)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.GeneratedMethodAccessor102.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-9220f1f9cf02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mbanktable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0mby\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \"\"\").show()\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input ')' expecting <EOF>(line 3, pos 70)\\n\\n== SQL ==\\n\\n    select age,marital, count(*) as total,\\n        (select count(*) from banktable where y = 'yes')/count(*) *100)\\n----------------------------------------------------------------------^^^\\n\\n    from banktable\\ngroup by age, marital\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select age,marital, count(*) as total,\n",
    "        (select count(*) from banktable where y = 'yes')/count(*) *100)\n",
    "\n",
    "    from banktable\n",
    "group by age, marital\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2560dce",
   "metadata": {},
   "source": [
    "#### 10. Do features engineering for Bank Investment scheme and find Effect of Age on the champaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55a4d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|age_category|subscriber_count|\n",
      "+------------+----------------+\n",
      "| Middle Aged|            2194|\n",
      "|       Adult|            1982|\n",
      "|         old|             906|\n",
      "|    Teenager|             207|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select age_category,count(*) as subscriber_count from (select case when age < 25 then 'Teenager'\n",
    "                when age >= 25 and age <= 35 then 'Adult'\n",
    "                when age > 35 and age <= 55 then 'Middle Aged'\n",
    "                else 'old' \n",
    "            end as age_category \n",
    "    from banktable where y = 'yes') group by age_category order by subscriber_count desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda67868",
   "metadata": {},
   "source": [
    "#### Alternative Method : Create UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7a9c3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50cb746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_range = udf(lambda age : 'Teenager' if age < 20 else \n",
    "                             'Adult' if (age >= 25 and age <= 35) else\n",
    "                            'Middle Aged' if (age >= 35 and age < 55) else\n",
    "                'Old'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb76c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_customer_DF = bank_customer_data.withColumn('age_category', age_range(bank_customer_data.age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9158822",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_customer_DF.createOrReplaceTempView('newbanktable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6757e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|age_category|success_count|\n",
      "+------------+-------------+\n",
      "| Middle Aged|        22598|\n",
      "|       Adult|        16098|\n",
      "|         Old|         6468|\n",
      "|    Teenager|           47|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select age_category, count(*) as success_count from newbanktable \n",
    "    group by age_category\n",
    "    order by success_count desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97684167",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+------------+\n",
      "|age|balance|campaign|contact|day|default|duration|education|housing|         job|loan| marital|month|pdays|poutcome|previous|  y|age_category|\n",
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+------------+\n",
      "| 58|   2143|       1|unknown|  5|     no|     261| tertiary|    yes|  management|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 44|     29|       1|unknown|  5|     no|     151|secondary|    yes|  technician|  no|  single|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 33|      2|       1|unknown|  5|     no|      76|secondary|    yes|entrepreneur| yes| married|  may|   -1| unknown|       0| no|       Adult|\n",
      "| 47|   1506|       1|unknown|  5|     no|      92|  unknown|    yes| blue-collar|  no| married|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 33|      1|       1|unknown|  5|     no|     198|  unknown|     no|     unknown|  no|  single|  may|   -1| unknown|       0| no|       Adult|\n",
      "| 35|    231|       1|unknown|  5|     no|     139| tertiary|    yes|  management|  no| married|  may|   -1| unknown|       0| no|       Adult|\n",
      "| 28|    447|       1|unknown|  5|     no|     217| tertiary|    yes|  management| yes|  single|  may|   -1| unknown|       0| no|       Adult|\n",
      "| 42|      2|       1|unknown|  5|    yes|     380| tertiary|    yes|entrepreneur|  no|divorced|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 58|    121|       1|unknown|  5|     no|      50|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 43|    593|       1|unknown|  5|     no|      55|secondary|    yes|  technician|  no|  single|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 41|    270|       1|unknown|  5|     no|     222|secondary|    yes|      admin.|  no|divorced|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 29|    390|       1|unknown|  5|     no|     137|secondary|    yes|      admin.|  no|  single|  may|   -1| unknown|       0| no|       Adult|\n",
      "| 53|      6|       1|unknown|  5|     no|     517|secondary|    yes|  technician|  no| married|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 58|     71|       1|unknown|  5|     no|      71|  unknown|    yes|  technician|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 57|    162|       1|unknown|  5|     no|     174|secondary|    yes|    services|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 51|    229|       1|unknown|  5|     no|     353|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 45|     13|       1|unknown|  5|     no|      98|  unknown|    yes|      admin.|  no|  single|  may|   -1| unknown|       0| no| Middle Aged|\n",
      "| 57|     52|       1|unknown|  5|     no|      38|  primary|    yes| blue-collar|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 60|     60|       1|unknown|  5|     no|     219|  primary|    yes|     retired|  no| married|  may|   -1| unknown|       0| no|         Old|\n",
      "| 33|      0|       1|unknown|  5|     no|      54|secondary|    yes|    services|  no| married|  may|   -1| unknown|       0| no|       Adult|\n",
      "+---+-------+--------+-------+---+-------+--------+---------+-------+------------+----+--------+-----+-----+--------+--------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_customer_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "088fa1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f0a98",
   "metadata": {},
   "source": [
    "#### write a query to display distributed probablity for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224030c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
